---
title: "Text analysis of news headlines"
output:
  github_document:
    toc: true
    dev: svg
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidytext)
library(lubridate)
library(countrycode)
```

```{r loadData}
reutersData <- 
  dir("data", full.names = T) %>% 
  # col_types = "cc" will read both columns as character
  # (by default the "publish_time" column would be read as integer.)
  map_df( read_csv,col_types = "cc") %>%
  # ymd_hm() will read date+time in the format [year][month][day][hour][minute]
  mutate(publish_time = ymd_hm(publish_time))
```


```{r, eval=F, echo=F}
# Work on a small subset of the data for testing.
# Remove this chunk to run on the entire dataset.

reutersDataFull <- reutersData

reutersData <- 
  reutersDataFull %>% 
  sample_n(100000)
```


## Feature engineering

### Counts

* Word Count - Total number of words in the headline
* Character Count - Total number of characters in the headline excluding spaces
* Mean Word Length - Average length of the words used in the headline
* Word Density - Words per character
* Punctuation Count - Total number of punctuations used in the headline
* Upper-Case to Lower-Case Words ratio - ratio of upper case words used and lower case words used in the text

```{r countStats, cache=T}
reutersData <-
  reutersData %>% 
  mutate( word_count = map_int(strsplit(headline_text, split= " "), length) ) %>%
  # mutate( word_count2 = nchar(gsub("[^ ]","",headline_text)) + 1 ) %>% # alternative
  mutate( character_count = nchar(gsub(" ","",headline_text)) ) %>%
  mutate( mean_word_length = character_count / word_count ) %>%
  mutate( word_density = word_count / (character_count + 1) ) %>%  # +1 because... it was in the kaggle solution
  mutate( punctuation_count = nchar(gsub("[^[:punct:]]","",headline_text)) )
```

### Countries

Get the first country mentioned in each headline. Using the countrycode which contains country names, regex expressions and country codes.

```{r countCountries, cache=T}
getCountries <- function(x){
  # look for countries in the text
  # Only the first country found in the text will be returned
  retCntry <- character(length(x))
  hitpos <- rep(Inf,length(x))
  # for each country
  for(i in 1:nrow(codelist)){
    cntry_regexpr <- regexpr(codelist$country.name.en.regex[i], 
                      x, ignore.case = T,perl=T)
    # which lines of texts has a hit
    idxHits <- na.omit(which(cntry_regexpr != -1))
    # Only keep hits if that are before a prior hit
    idxHits <- idxHits[cntry_regexpr[idxHits] < hitpos[idxHits]]
    # Store the country and position of the hits
    retCntry[idxHits] <- rep(codelist$iso3c[i],length(idxHits))
    hitpos[idxHits] <- cntry_regexpr[idxHits]
  }
  return(retCntry)
}

# split up the work and run in parallel using mclapply 
blockApply <- function(x, FUN, cores=3,blockSize=1000, ...){
  nBlocks <- ceiling(length(x)/blockSize)
  parallel::mclapply(1:nBlocks, mc.cores = cores, function(iBlock){
    i <- ((iBlock-1)*blockSize+1):min(iBlock*blockSize,length(x))
    FUN(x[i], ...)
  }) %>% 
    unlist
}

reutersData <-
  reutersData %>% 
  mutate( country = blockApply(reutersData$headline_text, getCountries, cores=4,blockSize = 10000) )

```

### Distaters

Look for specific words related to disters to classify each headline.

```{r getDisasters}

getCategory <- function(x, categories){
  # look for categories in the text
  # Only the first category found in the text will be returned
  tibble( idx = seq_along(x), text = x) %>% 
    unnest_tokens( word, text ) %>% 
    filter( word %in% categories) %>%
    distinct( idx, .keep_all = T ) %>% 
    right_join(tibble( idx = seq_along(x)), by="idx") %>% 
    with( word )
}


disasterWords <- c('hurricane', 'tornado', 'blizzerd', 'fire', 'earthquake', 'flood', 'cyclone', 'avalanche', 'drought', 'storm', 'lightning', 'tsunami')

reutersData <-
  reutersData %>% 
  mutate( disaster = getCategory(headline_text, disasterWords))

```



## Word usage

Bag-of-words: look for specific words and look for co-occurence with these

## Exploratory analysis


### Word count

```{r wordCountPlot}
reutersData %>% 
  mutate( year = year(publish_time) ) %>% 
  ggplot( aes(x=word_count) ) +
  geom_histogram(binwidth = 1) +
  facet_wrap( ~ year)
```

### Character count

```{r characterCountPlot}
reutersData %>% 
  mutate( year = factor(year(publish_time)) ) %>% 
  ggplot( aes(y=character_count, x = year) )+
  geom_violin( )
```

### Word length

```{r wordLengthPlot}
reutersData %>% 
  mutate( year = factor(year(publish_time)) ) %>% 
  ggplot( aes(y=mean_word_length, x = year) ) +
  geom_violin( )
```

### Time distribution

#### Number of headlines per day over the years

```{r perDayTrendPlot}
reutersData %>% 
  mutate( publish_day = floor_date(publish_time, unit = "day") ) %>% 
  group_by( publish_day ) %>% 
  summarise( headlinesPerDay = n()) %>%
  ggplot( aes( x = publish_day, y = headlinesPerDay) ) +
  geom_smooth( )
```

#### Month of the year

```{r monthPlot}
reutersData %>% 
  mutate( month_num = month(publish_time) ) %>%
  # convert month number to three letter abbrivation using built-in constant month.abb
  mutate( month = factor(month.abb[month_num], levels = month.abb) ) %>% 
  ggplot( aes( x = month ) ) +
  geom_bar()
```

#### Week of the year

```{r weekPlot}
reutersData %>% 
  ggplot( aes( x = as.integer(week(publish_time))) ) +
  geom_bar() +
  scale_x_continuous( breaks=c(1,(1:5)*10,53) )
```

#### Hour of the day

```{r hourPlot}
reutersData %>% 
  ggplot( aes( x = hour(publish_time)) ) +
  geom_histogram( bins = 24)
```

#### Minute of the hour

```{r minutePlot}
reutersData %>% 
  ggplot( aes( x = minute(publish_time)) ) +
  geom_histogram( bins = 60)
```

```{r}

reutersData %>% 
  count( h=hour(publish_time), m=minute(publish_time) ) %>% 
  arrange(-n) %>% 
  top_n(100, n) %>% 
  mutate(roundedTime = case_when(
    m==0 ~ "hour",
    m==30 ~ "half-hour",
    (m%%15)==0 ~ "quarter",
    (m%%5)==0 ~ "five-min",
    TRUE ~ "none")
  ) %>% 
  mutate( hourOfDay = h + m/60 ) %>% 
  ggplot( aes(x=hourOfDay,y=n, color=roundedTime)) +
  geom_point()

```


